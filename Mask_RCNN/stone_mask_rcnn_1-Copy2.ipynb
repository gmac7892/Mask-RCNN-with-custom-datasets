{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cai/anaconda3/envs/MRCNN/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/cai/anaconda3/envs/MRCNN/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/cai/anaconda3/envs/MRCNN/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/cai/anaconda3/envs/MRCNN/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/cai/anaconda3/envs/MRCNN/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/cai/anaconda3/envs/MRCNN/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from mrcnn.config import Config\n",
    "# from mrcnn import model as modellib\n",
    "# from mrcnn import visualize\n",
    "# import mrcnn\n",
    "# from mrcnn.utils import Dataset\n",
    "# from mrcnn.model import MaskRCNN\n",
    "# import numpy as np\n",
    "# from numpy import zeros\n",
    "# from numpy import asarray\n",
    "# import colorsys\n",
    "# import argparse\n",
    "# import imutils\n",
    "# import random\n",
    "# import cv2\n",
    "# import os\n",
    "# import time\n",
    "# from matplotlib import pyplot\n",
    "# from matplotlib.patches import Rectangle\n",
    "# from keras.models import load_model\n",
    "# %matplotlib inline\n",
    "# from os import listdir\n",
    "# from xml.etree import ElementTree\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "\n",
    "# Import Mask RCNN\n",
    "# sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one of the code blocks\n",
    "\n",
    "# Shapes toy dataset\n",
    "\n",
    "# import shapes\n",
    "# config = shapes.ShapesConfig()\n",
    "\n",
    "# MS COCO Dataset\n",
    "import coco\n",
    "config = coco.CocoConfig()\n",
    "COCO_DIR = os.path.abspath(r'../')\n",
    "\n",
    "# TODO: enter value here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# png_images = os.listdir(COCO_DIR + '/train2014/')\n",
    "# for png_image in png_images:\n",
    "    \n",
    "#     try:\n",
    "#         png = Image.open(COCO_DIR + '/train2014/' + str(png_image)[:-4] + '.png')\n",
    "#         png.save(COCO_DIR + '/train2014/' + str(png_image)[:-4] + '.jpg')\n",
    "#         os.remove(COCO_DIR + '/train2014/' + png_image)\n",
    "        \n",
    "#     except:\n",
    "#         continue\n",
    "        \n",
    "\n",
    "# png_images = os.listdir(COCO_DIR + '/val2014/')\n",
    "# for png_image in png_images:\n",
    "    \n",
    "#     try:\n",
    "#         png = Image.open(COCO_DIR + '/val2014/' + str(png_image)[:-4] + '.png')\n",
    "#         png.save(COCO_DIR + '/val2014/' + str(png_image)[:-4] + '.jpg')\n",
    "#         os.remove(COCO_DIR + '/val2014/' + png_image)\n",
    "        \n",
    "#     except:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Image Count: 31\n",
      "Class Count: 2\n",
      "  0. BG                                                \n",
      "  1. stone                                             \n",
      "Image Count: 4\n",
      "Class Count: 2\n",
      "  0. BG                                                \n",
      "  1. stone                                             \n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "if config.NAME == 'shapes':\n",
    "    dataset = shapes.ShapesDataset()\n",
    "    dataset.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "elif config.NAME == \"coco\":\n",
    "    dataset_train = coco.CocoDataset()\n",
    "    dataset_train.load_coco(COCO_DIR, \"train\")\n",
    "    dataset_val = coco.CocoDataset()\n",
    "    dataset_val.load_coco(COCO_DIR, \"val\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset_train.prepare()\n",
    "dataset_val.prepare()\n",
    "print(\"Image Count: {}\".format(len(dataset_train.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset_train.num_classes))\n",
    "for i, info in enumerate(dataset_train.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))\n",
    "    \n",
    "print(\"Image Count: {}\".format(len(dataset_val.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset_val.num_classes))\n",
    "for i, info in enumerate(dataset_val.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Samples\n",
    "\n",
    "Load and display images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Boxes\n",
    "\n",
    "Rather than using bounding box coordinates provided by the source datasets, we compute the bounding boxes from masks instead. This allows us to handle bounding boxes consistently regardless of the source dataset, and it also makes it easier to resize, rotate, or crop images because we simply generate the bounding boxes from the updates masks rather than computing bounding box transformation for each type of image transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mrcnn\n",
    "import logging\n",
    "\n",
    "# Load random image and mask.\n",
    "image_id = random.choice(dataset_train.image_ids)\n",
    "image = dataset_train.load_image(image_id)\n",
    "mask, class_ids = dataset_train.load_mask(image_id)\n",
    "# Compute Bounding box\n",
    "bbox = mrcnn.utils.extract_bboxes(mask)\n",
    "# Display image and additional stats\n",
    "print(\"image_id \", image_id, dataset_train.image_reference(image_id))\n",
    "log(\"image\", image)\n",
    "log(\"mask\", mask)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "# Display image and instances\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize Images\n",
    "\n",
    "To support multiple images per batch, images are resized to one size (1024x1024). Aspect ratio is preserved, though. If an image is not square, then zero padding is added at the top/bottom or right/left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load random image and mask.\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image = dataset.load_image(image_id)\n",
    "mask, class_ids = dataset.load_mask(image_id)\n",
    "original_shape = image.shape\n",
    "# Resize\n",
    "image, window, scale, padding, _ = utils.resize_image(\n",
    "    image, \n",
    "    min_dim=config.IMAGE_MIN_DIM, \n",
    "    max_dim=config.IMAGE_MAX_DIM,\n",
    "    mode=config.IMAGE_RESIZE_MODE)\n",
    "mask = utils.resize_mask(mask, scale, padding)\n",
    "# Compute Bounding box\n",
    "bbox = utils.extract_bboxes(mask)\n",
    "\n",
    "# Display image and additional stats\n",
    "print(\"image_id: \", image_id, dataset.image_reference(image_id))\n",
    "print(\"Original shape: \", original_shape)\n",
    "log(\"image\", image)\n",
    "log(\"mask\", mask)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "# Display image and instances\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "    dataset, config, image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"image\", image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "log(\"mask\", mask)\n",
    "\n",
    "display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add augmentation and mask resizing.\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "    dataset, config, image_id, augment=True, use_mini_mask=True)\n",
    "log(\"mask\", mask)\n",
    "display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mask = utils.expand_mask(bbox, mask, image.shape)\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/cai/Mask_RCNN\" \n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=40, \n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "\n",
    "\n",
    "class_names = ['stone']\n",
    "\n",
    "# Load a random image from the images folder\n",
    "IMAGE_DIR = \"/cai_lab/Sampyo_backup/crop_dataset_v6F/big/0\"\n",
    "\n",
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
    "\n",
    "# Run detection\n",
    "results = model.detect([image], verbose=1)\n",
    "\n",
    "# Visualize results\n",
    "r = results[0]\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                           ['BG', 'stone'], r['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coco\n",
    "# Load dataset\n",
    "if config.NAME == 'shapes':\n",
    "    dataset = shapes.ShapesDataset()\n",
    "    dataset.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "elif config.NAME == \"MaskRCNN_config\":\n",
    "    dataset = coco.CocoDataset()\n",
    "    dataset.load_coco(dataset_dir, \"train\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "for i, info in enumerate(dataset.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = dataset_dir + 'image_MRCNN'\n",
    "for filename in listdir(images_dir):\n",
    "    # extract image id\n",
    "    image_name = filename.split(\"_\")\n",
    "    image_id = (image_name[4])\n",
    "    annotations_dir = dataset_dir + 'annotation_MRCNN/'\n",
    "    ann_path = annotations_dir + \"_groundtruth_(1)_images_stone_1.png_\" + str(image_id[:-4])\n",
    "    print(ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoneDataset(Dataset):\n",
    "    # load the dataset definitions\n",
    "    def load_dataset(self, dataset_dir, is_train=True):\n",
    "        \n",
    "        # Add classes. We have only one class to add.\n",
    "        self.add_class(\"dataset\", 1, \"stone\")\n",
    "        \n",
    "        # define data locations for images and annotations\n",
    "        images_dir = dataset_dir + 'image_MRCNN/'\n",
    "        annotations_dir = dataset_dir + 'annotation_MRCNN/'\n",
    "        \n",
    "        # Iterate through all files in the folder to \n",
    "        #add class, images and annotaions\n",
    "        for filename in listdir(images_dir):\n",
    "            \n",
    "            # extract image id\n",
    "            image_name = filename.split(\"_\")\n",
    "            image_id = (image_name[4])\n",
    "#             # skip bad images\n",
    "#             if image_id in ['00090']:\n",
    "#                 continue\n",
    "#             # skip all images after 150 if we are building the train set\n",
    "#             if is_train and int(image_id) >= 150:\n",
    "#                 continue\n",
    "#             # skip all images before 150 if we are building the test/val set\n",
    "#             if not is_train and int(image_id) < 150:\n",
    "#                 continue\n",
    "            \n",
    "            # setting image file\n",
    "            img_path = images_dir + filename\n",
    "            \n",
    "            # setting annotations file\n",
    "            ann_path = annotations_dir + \"_groundtruth_(1)_images_stone_1.png_\" + str(image_id[:-4]) + '.json'\n",
    "            \n",
    "            # adding images and annotations to dataset\n",
    "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
    "# extract bounding boxes from an annotation file\n",
    "    def extract_boxes(self, filename):\n",
    "        \n",
    "        # load and parse the file\n",
    "        tree = ElementTree.parse(filename)\n",
    "        # get the root of the document\n",
    "        root = tree.getroot()\n",
    "        # extract each bounding box\n",
    "        boxes = list()\n",
    "        for box in root.findall('.//bndbox'):\n",
    "            xmin = int(box.find('xmin').text)\n",
    "            ymin = int(box.find('ymin').text)\n",
    "            xmax = int(box.find('xmax').text)\n",
    "            ymax = int(box.find('ymax').text)\n",
    "            coors = [xmin, ymin, xmax, ymax]\n",
    "            boxes.append(coors)\n",
    "        \n",
    "        # extract image dimensions\n",
    "        width = int(root.find('.//size/width').text)\n",
    "        height = int(root.find('.//size/height').text)\n",
    "        return boxes, width, height\n",
    "# load the masks for an image\n",
    "    \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "     \"\"\"\n",
    "    def load_mask(self, image_id):\n",
    "        # get details of image\n",
    "        info = self.image_info[image_id]\n",
    "        \n",
    "        # define anntation  file location\n",
    "        path = info['annotation']\n",
    "        \n",
    "        # load XML\n",
    "        boxes, w, h = self.extract_boxes(path)\n",
    "       \n",
    "        # create one array for all masks, each on a different channel\n",
    "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
    "        \n",
    "        # create masks\n",
    "        class_ids = list()\n",
    "        for i in range(len(boxes)):\n",
    "            box = boxes[i]\n",
    "            row_s, row_e = box[1], box[3]\n",
    "            col_s, col_e = box[0], box[2]\n",
    "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
    "            class_ids.append(self.class_names.index('kangaroo'))\n",
    "        return masks, asarray(class_ids, dtype='int32')\n",
    "# load an image reference\n",
    "     #\"\"\"Return the path of the image.\"\"\"\n",
    "    def image_reference(self, image_id):\n",
    "        info = self.image_info[image_id]\n",
    "        print(info)\n",
    "        return info['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train set\n",
    "train_set = StoneDataset()\n",
    "train_set.load_dataset(dataset_dir, is_train=True)\n",
    "train_set.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(train_set.image_ids)))\n",
    "print(\"Class Count: {}\".format(train_set.num_classes))\n",
    "for i, info in enumerate(train_set.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and display random samples\n",
    "image_ids = np.random.choice(train_set.image_ids, 1)\n",
    "for image_id in image_ids:\n",
    "    image = train_set.load_image(image_id)\n",
    "    mask, class_ids = train_set.load_mask(image_id)\n",
    "#     visualize.display_top_masks(image, mask, class_ids, train_set.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
